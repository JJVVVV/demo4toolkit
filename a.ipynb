{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def duplicate_rows(df, n):\n",
    "    df_list = [df] * n\n",
    "    duplicated_df = pd.concat(df_list, ignore_index=True)\n",
    "    return duplicated_df\n",
    "\n",
    "\n",
    "duplicated_df = duplicate_rows(df, 10)\n",
    "duplicated_df.to_json(\"data/fool/train/train_generate.json\", indent=2, force_ascii=False, orient=\"records\")\n",
    "df.to_json(\"data/fool/dev/dev_generate.json\", indent=2, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "for i in range(len(data[\"inputs\"])):\n",
    "    data[\"inputs\"][i] = data[\"inputs\"][i] + \", \" + data[\"outputs\"][i]\n",
    "    data[\"outputs\"][i] = 1\n",
    "df1 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "for i in range(len(data[\"inputs\"])):\n",
    "    data[\"inputs\"][i] = data[\"inputs\"][i] + \", \" + data[\"outputs\"][len(data[\"inputs\"]) - 1 - i]\n",
    "for i in range(len(data[\"outputs\"])):\n",
    "    data[\"outputs\"][i] = 0\n",
    "df2 = pd.DataFrame(data)\n",
    "df2\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "def duplicate_rows(df, n):\n",
    "    df_list = [df] * n\n",
    "    duplicated_df = pd.concat(df_list, ignore_index=True)\n",
    "    return duplicated_df\n",
    "\n",
    "\n",
    "duplicated_df = duplicate_rows(df, 10)\n",
    "duplicated_df.to_json(\"data/fool/train/train_classify.json\", indent=2, force_ascii=False, orient=\"records\")\n",
    "df.to_json(\"data/fool/dev/dev_classify.json\", indent=2, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(a, **kwargs_load_data):\n",
    "    print(kwargs_load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kwargs_load_data': 'abc'}\n"
     ]
    }
   ],
   "source": [
    "my_function(1, kwargs_load_data=\"abc\")\n",
    "# TypeError: my_function() got an unexpected keyword argument 'kwargs_load_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1: 1} or dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"{1: 1}\"下一\\'\\'句是?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"\\\"{a}\\\"下一''句是?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/Qwen/Qwen2.5-0.5B-Instruct/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(tk, \"apply_chat_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/google/mt5-small/\")\n",
    "hasattr(tk, \"chat_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['inputs', 'outputs'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['inputs', 'outputs'],\n",
      "        num_rows: 24\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 假设你已经加载了数据\n",
    "df_train = pd.read_json(\"data/fool/train/train_classify.json\", lines=False)\n",
    "df_dev = pd.read_json(\"data/fool/dev/dev_classify.json\", lines=False)\n",
    "\n",
    "# 将 Pandas DataFrame 转换为 Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "dev_dataset = Dataset.from_pandas(df_dev)\n",
    "\n",
    "# 可选：将训练集和验证集组合成一个 DatasetDict\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": dev_dataset})\n",
    "\n",
    "# 打印数据集信息\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'train_file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabc/def\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'train_file_path'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"your-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    train_file_path=\"abc/def\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"rouge\", cache_dir=\"outputs/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:59:24,284\u001b[32m <INFO> Configuration: 🔧 Custom attribute: deepspeed_config=None\u001b[0m\n",
      "2025-04-21 19:59:24,287\u001b[32m <INFO> Configuration: 🔧 Custom attribute: num_classification=2\u001b[0m\n",
      "2025-04-21 19:59:24,288\u001b[32m <INFO> Configuration: 🔧 Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-04-21 19:59:24,289\u001b[32m <INFO> Configuration: 🔧 Custom attribute: part=all\u001b[0m\n",
      "2025-04-21 19:59:24,290\u001b[32m <INFO> Configuration: 🔧 Custom attribute: hf_generation_config_file=./configs/generate_config.json\u001b[0m\n",
      "2025-04-21 19:59:24,436 <DEBUG> TextDataset: ⏳ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-04-21 19:59:24,437 <DEBUG> TextDataset: Model max length: 512\u001b[0m\n",
      "Index(['inputs', 'outputs'], dtype='object')\n",
      "==============================TRAINING==============================\n",
      "### Input: \n",
      "赵客缦胡缨, 吴钩霜雪明\n",
      "### Output: \n",
      "[1]\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a06a92667d245f184ba6c6d48662636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING input texts:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:59:24,578 <DEBUG> TextDataset: ⌛ Loading TRAINING data takes 0.14 sec.\u001b[0m\n",
      "2025-04-21 19:59:24,580\u001b[32m <INFO> [toolkit]: Total TRAINING data: 240\u001b[0m\n",
      "2025-04-21 19:59:24,581\u001b[32m <INFO> [toolkit]: Max length of input: 13\u001b[0m\n",
      "2025-04-21 19:59:24,581\u001b[32m <INFO> [toolkit]: Max length of label: 1\u001b[0m\n",
      "2025-04-21 19:59:24,585\u001b[32m <INFO> [toolkit]: ✂️  Truncating TRAINING data: cnt=0, input_len=13, label_len=1.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from toolkit.nlp import TextDataset, NLPTrainingConfig\n",
    "from utils.load_data_fn import load_data_fn4classify, load_data_fn4generate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = NLPTrainingConfig.load(\"outputs/fool/bert-base-chinese/ORI/train_classify-dev_classify-None/all/baseline/3/16/2e-05/2/train_config.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/bert-base-chinese/\")\n",
    "train_dataset = TextDataset.from_file(\n",
    "    tokenizer=tokenizer,\n",
    "    load_data_fn=load_data_fn4generate if config.task_type == \"generate\" else load_data_fn4classify,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_inputs': {'input_ids': [101, 6627, 2145, 100, 5529, 5366, 117, 1426, 7174, 7458, 7434, 3209, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <toolkit.nlp.data.TextDataset object at 0x7fbc23e150d0>\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "DatasetDict({\"train\": train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! find \"/data/jjwang/codes/learn2reason/outputs/SelQA\" -type d -name \"optimal_checkpoint\" -exec rm -rf {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 23:00:46,649\u001b[32m <INFO> Configuration: 🔧 Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-04-29 23:00:46,650\u001b[32m <INFO> NLPTrainingConfig: ⚙️  Auto setting `cut_input_from_output='True'` according to `model_structure=decoder`\u001b[0m\n",
      "2025-04-29 23:00:47,070 <DEBUG> TextDataset: ⏳ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-04-29 23:00:47,072 <DEBUG> TextDataset: Model max length: 131072\u001b[0m\n",
      "Index(['inputs', 'outputs'], dtype='object')\n",
      "==============================TRAINING==============================\n",
      "### Input: \n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\"赵客缦胡缨\"下一句是?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "### Output: \n",
      "吴钩霜雪明<|im_end|>\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c4aa19e6a44fb8b7451abf9c784946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING input texts:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a980405f214440688923e2814127ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING label texts:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-29 23:00:47,140 <DEBUG> TextDataset: ⌛ Loading TRAINING data takes 0.07 sec.\u001b[0m\n",
      "2025-04-29 23:00:47,141\u001b[32m <INFO> [toolkit]: Total TRAINING data: 120\u001b[0m\n",
      "2025-04-29 23:00:47,141\u001b[32m <INFO> [toolkit]: Max length of input: 40\u001b[0m\n",
      "2025-04-29 23:00:47,142\u001b[32m <INFO> [toolkit]: Max length of label: 6\u001b[0m\n",
      "2025-04-29 23:00:47,143\u001b[32m <INFO> [toolkit]: ✂️  Truncating TRAINING data: cnt=0, input_len=46, label_len=46.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from toolkit.nlp import TextDataset, NLPTrainingConfig\n",
    "from transformers import AutoTokenizer\n",
    "from utils.load_data_fn import load_data_fn4generate\n",
    "\n",
    "config = NLPTrainingConfig(task_type=\"generate\", model_structure=\"decoder\", padding_side=\"left\", use_cache=False, text_type=\"ORI\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/Qwen/Qwen2.5-0.5B-Instruct/\")\n",
    "dataset = TextDataset.from_file(\n",
    "    data_file_path=\"data/fool/train/train_generate.json\",\n",
    "    tokenizer=tokenizer,\n",
    "    load_data_fn=load_data_fn4generate,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[33m### Special tokens map:\u001b[0m \n",
      "{'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[91m### Token ids:\u001b[0m \n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,      1, 103959,  64754,\n",
      "         121538, 100693, 121540,      1,  16872, 104670,  20412,     30, 151645,\n",
      "            198, 151644,  77091,    198, 103948, 103316, 105401, 100167,  30858,\n",
      "         151645]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100, 103948, 103316, 105401, 100167,  30858,\n",
      "         151645]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[92m### Decoded input ids:\u001b[0m \n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\"赵客缦胡缨\"下一句是?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "吴钩霜雪明<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[94m### Decoded label ids:\u001b[0m \n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>吴钩霜雪明<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def show_model_inputs_case(dataset, tokenizer, is_decode_label=True):\n",
    "    BOLD = \"\\033[1m\"\n",
    "    RED = \"\\033[91m\"\n",
    "    GREEN = \"\\033[92m\"\n",
    "    BLUE = \"\\033[94m\"\n",
    "    YELLOW = \"\\033[33m\"\n",
    "    ORANGE_256 = \"\\033[38;5;208m\"\n",
    "    RESET = \"\\033[0m\"  # 重置为默认颜色\n",
    "    print(f\"{'-'*100}\\n{BOLD + YELLOW}### Special tokens map:{RESET} \\n{tokenizer.special_tokens_map}\\n{'-'*100}\")\n",
    "    dataloader = DataLoader(dataset, collate_fn=dataset.collate_fn)\n",
    "    a_batch = next(iter(dataloader))\n",
    "    print(f\"{BOLD + RED}### Token ids:{RESET} \\n{a_batch}\\n{'-'*100}\")\n",
    "    print(f\"{BOLD + GREEN}### Decoded input ids:{RESET} \\n{tokenizer.batch_decode(a_batch['input_ids'], skip_special_tokens=False)[0]}\\n{'-'*100}\")\n",
    "    if is_decode_label:\n",
    "        a_batch[\"labels\"] = torch.where(a_batch[\"labels\"] != -100, a_batch[\"labels\"], tokenizer.pad_token_id)\n",
    "        print(f\"{BOLD + BLUE}### Decoded label ids:{RESET} \\n{tokenizer.batch_decode(a_batch['labels'], skip_special_tokens=False)[0]}\\n{'-'*100}\")\n",
    "\n",
    "\n",
    "show_model_inputs_case(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
