{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def duplicate_rows(df, n):\n",
    "    df_list = [df] * n\n",
    "    duplicated_df = pd.concat(df_list, ignore_index=True)\n",
    "    return duplicated_df\n",
    "\n",
    "\n",
    "duplicated_df = duplicate_rows(df, 10)\n",
    "duplicated_df.to_json(\"data/fool/train/train_generate.json\", indent=2, force_ascii=False, orient=\"records\")\n",
    "df.to_json(\"data/fool/dev/dev_generate.json\", indent=2, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "for i in range(len(data[\"inputs\"])):\n",
    "    data[\"inputs\"][i] = data[\"inputs\"][i] + \", \" + data[\"outputs\"][i]\n",
    "    data[\"outputs\"][i] = 1\n",
    "df1 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "for i in range(len(data[\"inputs\"])):\n",
    "    data[\"inputs\"][i] = data[\"inputs\"][i] + \", \" + data[\"outputs\"][len(data[\"inputs\"]) - 1 - i]\n",
    "for i in range(len(data[\"outputs\"])):\n",
    "    data[\"outputs\"][i] = 0\n",
    "df2 = pd.DataFrame(data)\n",
    "df2\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "def duplicate_rows(df, n):\n",
    "    df_list = [df] * n\n",
    "    duplicated_df = pd.concat(df_list, ignore_index=True)\n",
    "    return duplicated_df\n",
    "\n",
    "\n",
    "duplicated_df = duplicate_rows(df, 10)\n",
    "duplicated_df.to_json(\"data/fool/train/train_classify.json\", indent=2, force_ascii=False, orient=\"records\")\n",
    "df.to_json(\"data/fool/dev/dev_classify.json\", indent=2, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(a, **kwargs_load_data):\n",
    "    print(kwargs_load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kwargs_load_data': 'abc'}\n"
     ]
    }
   ],
   "source": [
    "my_function(1, kwargs_load_data=\"abc\")\n",
    "# TypeError: my_function() got an unexpected keyword argument 'kwargs_load_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1: 1} or dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"{1: 1}\"下一\\'\\'句是?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"\\\"{a}\\\"下一''句是?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/Qwen/Qwen2.5-0.5B-Instruct/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(tk, \"apply_chat_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/google/mt5-small/\")\n",
    "hasattr(tk, \"chat_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['inputs', 'outputs'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['inputs', 'outputs'],\n",
      "        num_rows: 24\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 假设你已经加载了数据\n",
    "df_train = pd.read_json(\"data/fool/train/train_classify.json\", lines=False)\n",
    "df_dev = pd.read_json(\"data/fool/dev/dev_classify.json\", lines=False)\n",
    "\n",
    "# 将 Pandas DataFrame 转换为 Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "dev_dataset = Dataset.from_pandas(df_dev)\n",
    "\n",
    "# 可选：将训练集和验证集组合成一个 DatasetDict\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": dev_dataset})\n",
    "\n",
    "# 打印数据集信息\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'train_file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabc/def\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'train_file_path'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"your-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    train_file_path=\"abc/def\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"rouge\", cache_dir=\"outputs/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:59:24,284\u001b[32m <INFO> Configuration: 🔧 Custom attribute: deepspeed_config=None\u001b[0m\n",
      "2025-04-21 19:59:24,287\u001b[32m <INFO> Configuration: 🔧 Custom attribute: num_classification=2\u001b[0m\n",
      "2025-04-21 19:59:24,288\u001b[32m <INFO> Configuration: 🔧 Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-04-21 19:59:24,289\u001b[32m <INFO> Configuration: 🔧 Custom attribute: part=all\u001b[0m\n",
      "2025-04-21 19:59:24,290\u001b[32m <INFO> Configuration: 🔧 Custom attribute: hf_generation_config_file=./configs/generate_config.json\u001b[0m\n",
      "2025-04-21 19:59:24,436 <DEBUG> TextDataset: ⏳ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-04-21 19:59:24,437 <DEBUG> TextDataset: Model max length: 512\u001b[0m\n",
      "Index(['inputs', 'outputs'], dtype='object')\n",
      "==============================TRAINING==============================\n",
      "### Input: \n",
      "赵客缦胡缨, 吴钩霜雪明\n",
      "### Output: \n",
      "[1]\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a06a92667d245f184ba6c6d48662636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING input texts:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:59:24,578 <DEBUG> TextDataset: ⌛ Loading TRAINING data takes 0.14 sec.\u001b[0m\n",
      "2025-04-21 19:59:24,580\u001b[32m <INFO> [toolkit]: Total TRAINING data: 240\u001b[0m\n",
      "2025-04-21 19:59:24,581\u001b[32m <INFO> [toolkit]: Max length of input: 13\u001b[0m\n",
      "2025-04-21 19:59:24,581\u001b[32m <INFO> [toolkit]: Max length of label: 1\u001b[0m\n",
      "2025-04-21 19:59:24,585\u001b[32m <INFO> [toolkit]: ✂️  Truncating TRAINING data: cnt=0, input_len=13, label_len=1.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from toolkit.nlp import TextDataset, NLPTrainingConfig\n",
    "from utils.load_data_fn import load_data_fn4classify, load_data_fn4generate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = NLPTrainingConfig.load(\"outputs/fool/bert-base-chinese/ORI/train_classify-dev_classify-None/all/baseline/3/16/2e-05/2/train_config.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/bert-base-chinese/\")\n",
    "train_dataset = TextDataset.from_file(\n",
    "    tokenizer=tokenizer,\n",
    "    load_data_fn=load_data_fn4generate if config.task_type == \"generate\" else load_data_fn4classify,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_inputs': {'input_ids': [101, 6627, 2145, 100, 5529, 5366, 117, 1426, 7174, 7458, 7434, 3209, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <toolkit.nlp.data.TextDataset object at 0x7fbc23e150d0>\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "DatasetDict({\"train\": train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! find \"/data/jjwang/codes/learn2reason/outputs/SelQA\" -type d -name \"optimal_checkpoint\" -exec rm -rf {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 00:18:21,648\u001b[32m <INFO> Configuration: 🔧 Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-06-23 00:18:21,650\u001b[32m <INFO> NLPTrainingConfig: ⚙️  Auto setting `cut_input_from_output='True'` according to `model_structure=decoder`\u001b[0m\n",
      "2025-06-23 00:18:22,021 <DEBUG> TextDataset: ⏳ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-06-23 00:18:22,022 <DEBUG> TextDataset: Model max length: 131072\u001b[0m\n",
      "Index(['inputs', 'outputs'], dtype='object')\n",
      "==============================TRAINING==============================\n",
      "### Input: \n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\"赵客缦胡缨\"下一句是?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "### Output: \n",
      "吴钩霜雪明<|im_end|>\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0ba58396a84102b199a9c1c89f67da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING input texts:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3555da8839540469d53ea5575efba28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING label texts:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 00:18:22,094 <DEBUG> TextDataset: ⌛ Loading TRAINING data takes 0.07 sec.\u001b[0m\n",
      "2025-06-23 00:18:22,095\u001b[32m <INFO> [toolkit]: Total TRAINING data: 120\u001b[0m\n",
      "2025-06-23 00:18:22,096\u001b[32m <INFO> [toolkit]: Max length of input tokens: 40\u001b[0m\n",
      "2025-06-23 00:18:22,096\u001b[32m <INFO> [toolkit]: Max length of label tokens: 6\u001b[0m\n",
      "2025-06-23 00:18:22,097\u001b[32m <INFO> [toolkit]: ✂️  Truncating TRAINING data: cnt=0, input_len=46, label_len=46.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from toolkit.nlp import TextDataset, NLPTrainingConfig\n",
    "from transformers import AutoTokenizer\n",
    "from utils.load_data_fn import load_data_fn4generate\n",
    "\n",
    "config = NLPTrainingConfig(task_type=\"generate\", model_structure=\"decoder\", padding_side=\"left\", use_cache=False, text_type=\"ORI\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/Qwen/Qwen2.5-0.5B-Instruct/\")\n",
    "dataset = TextDataset.from_file(\n",
    "    data_file_path=\"data/fool/train/train_generate.json\",\n",
    "    tokenizer=tokenizer,\n",
    "    load_data_fn=load_data_fn4generate,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_inputs': {'input_ids': [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 1, 103959, 64754, 121538, 100693, 121540, 1, 16872, 104670, 20412, 30, 151645, 198, 151644, 77091, 198, 103948, 103316, 105401, 100167, 30858, 151645], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103948, 103316, 105401, 100167, 30858, 151645]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,      1, 103959,  64754,\n",
      "         121538, 100693, 121540,      1,  16872, 104670,  20412,     30, 151645,\n",
      "            198, 151644,  77091,    198, 103948, 103316, 105401, 100167,  30858,\n",
      "         151645],\n",
      "        [151643, 151643, 151644,   8948,    198,   2610,    525,   1207,  16948,\n",
      "             11,   3465,    553,  54364,  14817,     13,   1446,    525,    264,\n",
      "          10950,  17847,     13, 151645,    198, 151644,    872,    198,      1,\n",
      "          99660, 102687,  99331, 115866,      1,  16872, 104670,  20412,     30,\n",
      "         151645,    198, 151644,  77091,    198, 120207, 119888,  29524, 118471,\n",
      "         151645],\n",
      "        [151643, 151643, 151643, 151644,   8948,    198,   2610,    525,   1207,\n",
      "          16948,     11,   3465,    553,  54364,  14817,     13,   1446,    525,\n",
      "            264,  10950,  17847,     13, 151645,    198, 151644,    872,    198,\n",
      "              1,  94498,  64682,  99733, 104855,      1,  16872, 104670,  20412,\n",
      "             30, 151645,    198, 151644,  77091,    198, 107903, 113869,  22243,\n",
      "         151645],\n",
      "        [151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,      1,  29826,  34187,\n",
      "         112419,  99741,  85336,      1,  16872, 104670,  20412,     30, 151645,\n",
      "            198, 151644,  77091,    198,  99194,  50366,  95256,  57218,  13072,\n",
      "         151645]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100, 103948, 103316, 105401, 100167,  30858,\n",
      "         151645],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100, 120207, 119888,  29524, 118471,\n",
      "         151645],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100, 107903, 113869,  22243,\n",
      "         151645],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,  99194,  50366,  95256,  57218,  13072,\n",
      "         151645]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(dataset, 4, collate_fn=train_dataset.collate_fn)\n",
    "for batch in dl:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[33m### Special tokens map:\u001b[0m \n",
      "{'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[91m### Token ids:\u001b[0m \n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,      1, 103959,  64754,\n",
      "         121538, 100693, 121540,      1,  16872, 104670,  20412,     30, 151645,\n",
      "            198, 151644,  77091,    198, 103948, 103316, 105401, 100167,  30858,\n",
      "         151645]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100, 103948, 103316, 105401, 100167,  30858,\n",
      "         151645]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[92m### Decoded input ids:\u001b[0m \n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\"赵客缦胡缨\"下一句是?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "吴钩霜雪明<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[94m### Decoded label ids:\u001b[0m \n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>吴钩霜雪明<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def show_model_inputs_case(dataset, tokenizer, is_decode_label=True):\n",
    "    BOLD = \"\\033[1m\"\n",
    "    RED = \"\\033[91m\"\n",
    "    GREEN = \"\\033[92m\"\n",
    "    BLUE = \"\\033[94m\"\n",
    "    YELLOW = \"\\033[33m\"\n",
    "    ORANGE_256 = \"\\033[38;5;208m\"\n",
    "    RESET = \"\\033[0m\"  # 重置为默认颜色\n",
    "    print(f\"{'-'*100}\\n{BOLD + YELLOW}### Special tokens map:{RESET} \\n{tokenizer.special_tokens_map}\\n{'-'*100}\")\n",
    "    dataloader = DataLoader(dataset, collate_fn=dataset.collate_fn)\n",
    "    a_batch = next(iter(dataloader))\n",
    "    print(f\"{BOLD + RED}### Token ids:{RESET} \\n{a_batch}\\n{'-'*100}\")\n",
    "    print(f\"{BOLD + GREEN}### Decoded input ids:{RESET} \\n{tokenizer.batch_decode(a_batch['input_ids'], skip_special_tokens=False)[0]}\\n{'-'*100}\")\n",
    "    if is_decode_label:\n",
    "        a_batch[\"labels\"] = torch.where(a_batch[\"labels\"] != -100, a_batch[\"labels\"], tokenizer.pad_token_id)\n",
    "        print(f\"{BOLD + BLUE}### Decoded label ids:{RESET} \\n{tokenizer.batch_decode(a_batch['labels'], skip_special_tokens=False)[0]}\\n{'-'*100}\")\n",
    "\n",
    "\n",
    "show_model_inputs_case(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试超大数据集读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"data/fool/train/train_generate.json\")\n",
    "# file_path = Path(\"data/fool/dev/dev_generate.json\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "with file_path.with_suffix(\".txt\").open(\"w\") as f:\n",
    "    for line in data:\n",
    "        f.write(f\"{line['inputs']}\\t{line['outputs']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.file = open(file_path, \"rb\")\n",
    "        self.mm = mmap.mmap(self.file.fileno(), 0, access=mmap.ACCESS_READ)\n",
    "\n",
    "        # 尝试加载预存的偏移量，否则重新计算\n",
    "        offsets_path = Path(file_path).with_suffix(\".offsets.npy\")\n",
    "        if offsets_path.exists():\n",
    "            self.offsets = np.load(offsets_path).tolist()\n",
    "        else:\n",
    "            self.offsets = self._build_offsets()\n",
    "            # np.save(offsets_path, np.array(self.offsets))\n",
    "\n",
    "    def _build_offsets(self):\n",
    "        offsets = []\n",
    "        self.mm.seek(0)\n",
    "        while True:\n",
    "            offsets.append(self.mm.tell())\n",
    "            if not self.mm.readline():\n",
    "                break\n",
    "        return offsets\n",
    "\n",
    "    def _build_offsets(self):\n",
    "        offsets = []\n",
    "        self.mm.seek(0)\n",
    "        while True:\n",
    "            pos = self.mm.tell()\n",
    "            line = self.mm.readline()\n",
    "            if not line:  # 遇到EOF时退出循环\n",
    "                print(line)\n",
    "                break\n",
    "            # if line.strip():  # 可选：忽略纯空白行（如只有\\n）\n",
    "            offsets.append(pos)\n",
    "        return offsets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.mm.seek(self.offsets[idx])\n",
    "        return self.mm.readline().decode(\"utf-8\").strip()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.mm.close()\n",
    "        self.file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n",
      "240\n",
      "赵客缦胡缨, 吴钩霜雪明\t1\n",
      "谁能书阁下, 吴钩霜雪明\t0\n"
     ]
    }
   ],
   "source": [
    "file_path = Path(\"data/fool/train/train_classify.txt\")\n",
    "fr = FileReader(file_path)\n",
    "print(len(fr.offsets))\n",
    "print(fr[0])\n",
    "print(fr[239])\n",
    "# print(fr[240])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 00:28:23,416\u001b[32m <INFO> Configuration: 🔧 Custom attribute: deepspeed_config=None\u001b[0m\n",
      "2025-06-23 00:28:23,417\u001b[32m <INFO> Configuration: 🔧 Custom attribute: num_classification=2\u001b[0m\n",
      "2025-06-23 00:28:23,418\u001b[32m <INFO> Configuration: 🔧 Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-06-23 00:28:23,419\u001b[32m <INFO> Configuration: 🔧 Custom attribute: part=all\u001b[0m\n",
      "2025-06-23 00:28:23,419\u001b[32m <INFO> Configuration: 🔧 Custom attribute: hf_generation_config_file=./configs/generate_config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 00:28:23,515 <DEBUG> TextDataset: ⏳ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-06-23 00:28:23,516 <DEBUG> TextDataset: Model max length: 512\u001b[0m\n",
      "Index(['inputs', 'outputs'], dtype='object')\n",
      "==============================TRAINING==============================\n",
      "### Input: \n",
      "赵客缦胡缨, 吴钩霜雪明\n",
      "### Output: \n",
      "[1]\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bc192af888491d8952f4652265cbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING input texts:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 00:28:23,575 <DEBUG> TextDataset: ⌛ Loading TRAINING data takes 0.06 sec.\u001b[0m\n",
      "2025-06-23 00:28:23,576\u001b[32m <INFO> [toolkit]: Total TRAINING data: 240\u001b[0m\n",
      "2025-06-23 00:28:23,577\u001b[32m <INFO> [toolkit]: Max length of input tokens: 13\u001b[0m\n",
      "2025-06-23 00:28:23,578\u001b[32m <INFO> [toolkit]: Max length of label tokens: 1\u001b[0m\n",
      "2025-06-23 00:28:23,580\u001b[32m <INFO> [toolkit]: ✂️  Truncating TRAINING data: cnt=240, input_len=5, label_len=1.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_inputs': {'input_ids': [101, 6627, 2145, 100, 5529], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]},\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit.nlp import TextDataset, NLPTrainingConfig\n",
    "from utils.load_data_fn import load_data_fn4classify, load_data_fn4generate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = NLPTrainingConfig.load(\"outputs/fool/bert-base-chinese/ORI/train_classify-dev_classify-None/all/baseline/3/16/2e-05/2/train_config.json\")\n",
    "config.max_length_input = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/bert-base-chinese/\")\n",
    "train_dataset = TextDataset.from_file(\n",
    "    tokenizer=tokenizer,\n",
    "    load_data_fn=load_data_fn4generate if config.task_type == \"generate\" else load_data_fn4classify,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 00:36:20,602\u001b[32m <INFO> Configuration: 🔧 Custom attribute: deepspeed_config=None\u001b[0m\n",
      "2025-06-23 00:36:20,603\u001b[32m <INFO> Configuration: 🔧 Custom attribute: num_classification=2\u001b[0m\n",
      "2025-06-23 00:36:20,604\u001b[32m <INFO> Configuration: 🔧 Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-06-23 00:36:20,604\u001b[32m <INFO> Configuration: 🔧 Custom attribute: part=all\u001b[0m\n",
      "2025-06-23 00:36:20,605\u001b[32m <INFO> Configuration: 🔧 Custom attribute: hf_generation_config_file=./configs/generate_config.json\u001b[0m\n",
      "2025-06-23 00:36:20,652 <DEBUG> TextDataset: ⏳ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-06-23 00:36:20,653 <DEBUG> TextDataset: ⌛ Loading TRAINING data takes 0.00 sec.\u001b[0m\n",
      "2025-06-23 00:36:20,654\u001b[32m <INFO> [toolkit]: Total TRAINING data: 240\u001b[0m\n",
      "2025-06-23 00:36:20,655\u001b[32m <INFO> [toolkit]: Max length of input tokens: -1\u001b[0m\n",
      "2025-06-23 00:36:20,655\u001b[32m <INFO> [toolkit]: Max length of label tokens: -1\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_inputs': {'input_ids': [101, 1282, 3635, 3324, 671, 782, 117, 1283, 7027, 679, 4522, 6121, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit.nlp import LazyTextDataset, NLPTrainingConfig\n",
    "from toolkit.nlp.data import show_model_inputs_case\n",
    "from utils.parse_item_fn import parse_item_fn4classify, parse_item_fn4generate\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "config = NLPTrainingConfig.load(\"outputs/fool/bert-base-chinese/ORI/train_classify-dev_classify-None/all/baseline/3/16/2e-05/2/train_config.json\")\n",
    "config.max_length_input = 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/bert-base-chinese/\")\n",
    "file_path = Path(\"data/fool/train/train_classify.txt\")\n",
    "train_dataset = LazyTextDataset.from_file(\n",
    "    data_file_path=file_path,\n",
    "    tokenizer=tokenizer,\n",
    "    parse_item_fn=parse_item_fn4generate if config.task_type == \"generate\" else parse_item_fn4classify,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")\n",
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[33m### Special tokens map:\u001b[0m \n",
      "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[91m### Token ids:\u001b[0m \n",
      "{'input_ids': tensor([[ 101, 6627, 2145,  100, 5529]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'labels': tensor([[1]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[92m### Decoded input ids:\u001b[0m \n",
      "[CLS] 赵 客 [UNK] 胡\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[94m### Decoded label ids:\u001b[0m \n",
      "[unused1]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "show_model_inputs_case(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 6627, 2145,  100, 5529],\n",
      "        [ 101, 7213, 7491, 4212, 4635],\n",
      "        [ 101, 1282, 3635, 3324,  671],\n",
      "        [ 101,  752,  749, 2855, 6132]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]]), 'labels': tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(train_dataset, 4, collate_fn=train_dataset.collate_fn)\n",
    "for batch in dl:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试生成任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-23 00:44:06,801\u001b[32m <INFO> Configuration: 🔧 Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-06-23 00:44:06,803\u001b[32m <INFO> NLPTrainingConfig: ⚙️  Auto setting `padding_side='left'` according to `model_structure=decoder`\u001b[0m\n",
      "2025-06-23 00:44:06,804\u001b[32m <INFO> NLPTrainingConfig: ⚙️  Auto setting `cut_input_from_output='True'` according to `model_structure=decoder`\u001b[0m\n",
      "2025-06-23 00:44:07,163 <DEBUG> TextDataset: ⏳ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-06-23 00:44:07,164 <DEBUG> TextDataset: ⌛ Loading TRAINING data takes 0.00 sec.\u001b[0m\n",
      "2025-06-23 00:44:07,165\u001b[32m <INFO> [toolkit]: Total TRAINING data: 120\u001b[0m\n",
      "2025-06-23 00:44:07,166\u001b[32m <INFO> [toolkit]: Max length of input tokens: -1\u001b[0m\n",
      "2025-06-23 00:44:07,166\u001b[32m <INFO> [toolkit]: Max length of label tokens: -1\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_inputs': {'input_ids': [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 1, 103959, 64754, 121538, 100693, 121540, 1, 16872, 104670, 20412, 30, 151645, 198, 151644, 77091, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " 'labels': [103948, 103316, 105401, 100167, 30858, 151645]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toolkit.nlp import LazyTextDataset, NLPTrainingConfig\n",
    "from toolkit.nlp.data import show_model_inputs_case\n",
    "from utils.parse_item_fn import parse_item_fn4classify, parse_item_fn4generate\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "config = NLPTrainingConfig(task_type=\"generate\", model_structure=\"decoder\", use_cache=False, text_type=\"ORI\")\n",
    "config.max_length = 44\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/Qwen/Qwen2.5-0.5B-Instruct/\")\n",
    "file_path = Path(\"data/fool/train/train_generate.txt\")\n",
    "train_dataset = LazyTextDataset.from_file(\n",
    "    data_file_path=file_path,\n",
    "    tokenizer=tokenizer,\n",
    "    parse_item_fn=parse_item_fn4generate if config.task_type == \"generate\" else parse_item_fn4classify,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吴钩霜雪明<|im_end|>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([103948, 103316, 105401, 100167, 30858, 151645])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[33m### Special tokens map:\u001b[0m \n",
      "{'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[91m### Token ids:\u001b[0m \n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,      1, 103959,  64754,\n",
      "         121538, 100693, 121540,      1,  16872, 104670,  20412,     30, 151645,\n",
      "            198, 151644,  77091,    198, 103948, 103316, 105401, 100167]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100, 103948, 103316, 105401, 100167]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[92m### Decoded input ids:\u001b[0m \n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\"赵客缦胡缨\"下一句是?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "吴钩霜雪\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\u001b[1m\u001b[94m### Decoded label ids:\u001b[0m \n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>吴钩霜雪\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "show_model_inputs_case(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,      1, 103959,  64754,\n",
      "         121538, 100693, 121540,      1,  16872, 104670,  20412,     30, 151645,\n",
      "            198, 151644,  77091,    198, 103948, 103316, 105401, 100167],\n",
      "        [151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,      1,  99660, 102687,\n",
      "          99331, 115866,      1,  16872, 104670,  20412,     30, 151645,    198,\n",
      "         151644,  77091,    198, 120207, 119888,  29524, 118471, 151645],\n",
      "        [151643, 151644,   8948,    198,   2610,    525,   1207,  16948,     11,\n",
      "           3465,    553,  54364,  14817,     13,   1446,    525,    264,  10950,\n",
      "          17847,     13, 151645,    198, 151644,    872,    198,      1,  94498,\n",
      "          64682,  99733, 104855,      1,  16872, 104670,  20412,     30, 151645,\n",
      "            198, 151644,  77091,    198, 107903, 113869,  22243, 151645],\n",
      "        [151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,      1,  29826,  34187,\n",
      "         112419,  99741,  85336,      1,  16872, 104670,  20412,     30, 151645,\n",
      "            198, 151644,  77091,    198,  99194,  50366,  95256,  57218]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100, 103948, 103316, 105401, 100167],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100, 120207, 119888,  29524, 118471, 151645],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100, 107903, 113869,  22243, 151645],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,  99194,  50366,  95256,  57218]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(train_dataset, 4, collate_fn=train_dataset.collate_fn)\n",
    "for batch in dl:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python操作耗时对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple: 0.138049 seconds\n",
      "List:  0.408929 seconds\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "tuple_time = timeit(\"t = (1,)\", number=10_000_000)\n",
    "list_time = timeit(\"l = [1]\", number=10_000_000)\n",
    "\n",
    "print(f\"Tuple: {tuple_time:.6f} seconds\")\n",
    "print(f\"List:  {list_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
