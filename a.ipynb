{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"èµµå®¢ç¼¦èƒ¡ç¼¨\",\n",
    "        \"é“¶éç…§ç™½é©¬\",\n",
    "        \"åæ­¥æ€ä¸€äºº\",\n",
    "        \"äº‹äº†æ‹‚è¡£å»\",\n",
    "        \"é—²è¿‡ä¿¡é™µé¥®\",\n",
    "        \"å°†ç‚™å•–æœ±äº¥\",\n",
    "        \"ä¸‰æ¯åç„¶è¯º\",\n",
    "        \"çœ¼èŠ±è€³çƒ­å\",\n",
    "        \"æ•‘èµµæŒ¥é‡‘æ§Œ\",\n",
    "        \"åƒç§‹äºŒå£®å£«\",\n",
    "        \"çºµæ­»ä¾ éª¨é¦™\",\n",
    "        \"è°èƒ½ä¹¦é˜ä¸‹\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"å´é’©éœœé›ªæ˜\",\n",
    "        \"é£’æ²“å¦‚æµæ˜Ÿ\",\n",
    "        \"åƒé‡Œä¸ç•™è¡Œ\",\n",
    "        \"æ·±è—èº«ä¸å\",\n",
    "        \"è„±å‰‘è†å‰æ¨ª\",\n",
    "        \"æŒè§åŠä¾¯å¬´\",\n",
    "        \"äº”å²³å€’ä¸ºè½»\",\n",
    "        \"æ„æ°”ç´ éœ“ç”Ÿ\",\n",
    "        \"é‚¯éƒ¸å…ˆéœ‡æƒŠ\",\n",
    "        \"çƒœèµ«å¤§æ¢åŸ\",\n",
    "        \"ä¸æƒ­ä¸–ä¸Šè‹±\",\n",
    "        \"ç™½é¦–å¤ªç„ç»\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def duplicate_rows(df, n):\n",
    "    df_list = [df] * n\n",
    "    duplicated_df = pd.concat(df_list, ignore_index=True)\n",
    "    return duplicated_df\n",
    "\n",
    "\n",
    "duplicated_df = duplicate_rows(df, 10)\n",
    "duplicated_df.to_json(\"data/fool/train/train_generate.json\", indent=2, force_ascii=False, orient=\"records\")\n",
    "df.to_json(\"data/fool/dev/dev_generate.json\", indent=2, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"èµµå®¢ç¼¦èƒ¡ç¼¨\",\n",
    "        \"é“¶éç…§ç™½é©¬\",\n",
    "        \"åæ­¥æ€ä¸€äºº\",\n",
    "        \"äº‹äº†æ‹‚è¡£å»\",\n",
    "        \"é—²è¿‡ä¿¡é™µé¥®\",\n",
    "        \"å°†ç‚™å•–æœ±äº¥\",\n",
    "        \"ä¸‰æ¯åç„¶è¯º\",\n",
    "        \"çœ¼èŠ±è€³çƒ­å\",\n",
    "        \"æ•‘èµµæŒ¥é‡‘æ§Œ\",\n",
    "        \"åƒç§‹äºŒå£®å£«\",\n",
    "        \"çºµæ­»ä¾ éª¨é¦™\",\n",
    "        \"è°èƒ½ä¹¦é˜ä¸‹\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"å´é’©éœœé›ªæ˜\",\n",
    "        \"é£’æ²“å¦‚æµæ˜Ÿ\",\n",
    "        \"åƒé‡Œä¸ç•™è¡Œ\",\n",
    "        \"æ·±è—èº«ä¸å\",\n",
    "        \"è„±å‰‘è†å‰æ¨ª\",\n",
    "        \"æŒè§åŠä¾¯å¬´\",\n",
    "        \"äº”å²³å€’ä¸ºè½»\",\n",
    "        \"æ„æ°”ç´ éœ“ç”Ÿ\",\n",
    "        \"é‚¯éƒ¸å…ˆéœ‡æƒŠ\",\n",
    "        \"çƒœèµ«å¤§æ¢åŸ\",\n",
    "        \"ä¸æƒ­ä¸–ä¸Šè‹±\",\n",
    "        \"ç™½é¦–å¤ªç„ç»\",\n",
    "    ],\n",
    "}\n",
    "for i in range(len(data[\"inputs\"])):\n",
    "    data[\"inputs\"][i] = data[\"inputs\"][i] + \", \" + data[\"outputs\"][i]\n",
    "    data[\"outputs\"][i] = 1\n",
    "df1 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"èµµå®¢ç¼¦èƒ¡ç¼¨\",\n",
    "        \"é“¶éç…§ç™½é©¬\",\n",
    "        \"åæ­¥æ€ä¸€äºº\",\n",
    "        \"äº‹äº†æ‹‚è¡£å»\",\n",
    "        \"é—²è¿‡ä¿¡é™µé¥®\",\n",
    "        \"å°†ç‚™å•–æœ±äº¥\",\n",
    "        \"ä¸‰æ¯åç„¶è¯º\",\n",
    "        \"çœ¼èŠ±è€³çƒ­å\",\n",
    "        \"æ•‘èµµæŒ¥é‡‘æ§Œ\",\n",
    "        \"åƒç§‹äºŒå£®å£«\",\n",
    "        \"çºµæ­»ä¾ éª¨é¦™\",\n",
    "        \"è°èƒ½ä¹¦é˜ä¸‹\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"å´é’©éœœé›ªæ˜\",\n",
    "        \"é£’æ²“å¦‚æµæ˜Ÿ\",\n",
    "        \"åƒé‡Œä¸ç•™è¡Œ\",\n",
    "        \"æ·±è—èº«ä¸å\",\n",
    "        \"è„±å‰‘è†å‰æ¨ª\",\n",
    "        \"æŒè§åŠä¾¯å¬´\",\n",
    "        \"äº”å²³å€’ä¸ºè½»\",\n",
    "        \"æ„æ°”ç´ éœ“ç”Ÿ\",\n",
    "        \"é‚¯éƒ¸å…ˆéœ‡æƒŠ\",\n",
    "        \"çƒœèµ«å¤§æ¢åŸ\",\n",
    "        \"ä¸æƒ­ä¸–ä¸Šè‹±\",\n",
    "        \"ç™½é¦–å¤ªç„ç»\",\n",
    "    ],\n",
    "}\n",
    "for i in range(len(data[\"inputs\"])):\n",
    "    data[\"inputs\"][i] = data[\"inputs\"][i] + \", \" + data[\"outputs\"][len(data[\"inputs\"]) - 1 - i]\n",
    "for i in range(len(data[\"outputs\"])):\n",
    "    data[\"outputs\"][i] = 0\n",
    "df2 = pd.DataFrame(data)\n",
    "df2\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "def duplicate_rows(df, n):\n",
    "    df_list = [df] * n\n",
    "    duplicated_df = pd.concat(df_list, ignore_index=True)\n",
    "    return duplicated_df\n",
    "\n",
    "\n",
    "duplicated_df = duplicate_rows(df, 10)\n",
    "duplicated_df.to_json(\"data/fool/train/train_classify.json\", indent=2, force_ascii=False, orient=\"records\")\n",
    "df.to_json(\"data/fool/dev/dev_classify.json\", indent=2, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(a, **kwargs_load_data):\n",
    "    print(kwargs_load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kwargs_load_data': 'abc'}\n"
     ]
    }
   ],
   "source": [
    "my_function(1, kwargs_load_data=\"abc\")\n",
    "# TypeError: my_function() got an unexpected keyword argument 'kwargs_load_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1: 1} or dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"{1: 1}\"ä¸‹ä¸€\\'\\'å¥æ˜¯?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"\\\"{a}\\\"ä¸‹ä¸€''å¥æ˜¯?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/Qwen/Qwen2.5-0.5B-Instruct/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['inputs', 'outputs'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['inputs', 'outputs'],\n",
      "        num_rows: 24\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# å‡è®¾ä½ å·²ç»åŠ è½½äº†æ•°æ®\n",
    "df_train = pd.read_json(\"data/fool/train/train_classify.json\", lines=False)\n",
    "df_dev = pd.read_json(\"data/fool/dev/dev_classify.json\", lines=False)\n",
    "\n",
    "# å°† Pandas DataFrame è½¬æ¢ä¸º Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "dev_dataset = Dataset.from_pandas(df_dev)\n",
    "\n",
    "# å¯é€‰ï¼šå°†è®­ç»ƒé›†å’ŒéªŒè¯é›†ç»„åˆæˆä¸€ä¸ª DatasetDict\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": dev_dataset})\n",
    "\n",
    "# æ‰“å°æ•°æ®é›†ä¿¡æ¯\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'train_file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabc/def\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'train_file_path'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"your-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    train_file_path=\"abc/def\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"rouge\", cache_dir=\"outputs/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:59:24,284\u001b[32m <INFO> Configuration: ğŸ”§ Custom attribute: deepspeed_config=None\u001b[0m\n",
      "2025-04-21 19:59:24,287\u001b[32m <INFO> Configuration: ğŸ”§ Custom attribute: num_classification=2\u001b[0m\n",
      "2025-04-21 19:59:24,288\u001b[32m <INFO> Configuration: ğŸ”§ Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-04-21 19:59:24,289\u001b[32m <INFO> Configuration: ğŸ”§ Custom attribute: part=all\u001b[0m\n",
      "2025-04-21 19:59:24,290\u001b[32m <INFO> Configuration: ğŸ”§ Custom attribute: hf_generation_config_file=./configs/generate_config.json\u001b[0m\n",
      "2025-04-21 19:59:24,436 <DEBUG> TextDataset: â³ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-04-21 19:59:24,437 <DEBUG> TextDataset: Model max length: 512\u001b[0m\n",
      "Index(['inputs', 'outputs'], dtype='object')\n",
      "==============================TRAINING==============================\n",
      "### Input: \n",
      "èµµå®¢ç¼¦èƒ¡ç¼¨, å´é’©éœœé›ªæ˜\n",
      "### Output: \n",
      "[1]\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a06a92667d245f184ba6c6d48662636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING input texts:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:59:24,578 <DEBUG> TextDataset: âŒ› Loading TRAINING data takes 0.14 sec.\u001b[0m\n",
      "2025-04-21 19:59:24,580\u001b[32m <INFO> [toolkit]: Total TRAINING data: 240\u001b[0m\n",
      "2025-04-21 19:59:24,581\u001b[32m <INFO> [toolkit]: Max length of input: 13\u001b[0m\n",
      "2025-04-21 19:59:24,581\u001b[32m <INFO> [toolkit]: Max length of label: 1\u001b[0m\n",
      "2025-04-21 19:59:24,585\u001b[32m <INFO> [toolkit]: âœ‚ï¸  Truncating TRAINING data: cnt=0, input_len=13, label_len=1.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from toolkit.nlp import TextDataset, NLPTrainingConfig\n",
    "from utils.load_data_fn import load_data_fn4classify, load_data_fn4generate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = NLPTrainingConfig.load(\"outputs/fool/bert-base-chinese/ORI/train_classify-dev_classify-None/all/baseline/3/16/2e-05/2/train_config.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/bert-base-chinese/\")\n",
    "train_dataset = TextDataset.from_file(\n",
    "    tokenizer=tokenizer,\n",
    "    load_data_fn=load_data_fn4generate if config.task_type == \"generate\" else load_data_fn4classify,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_inputs': {'input_ids': [101, 6627, 2145, 100, 5529, 5366, 117, 1426, 7174, 7458, 7434, 3209, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <toolkit.nlp.data.TextDataset object at 0x7fbc23e150d0>\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "DatasetDict({\"train\": train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
