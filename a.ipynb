{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def duplicate_rows(df, n):\n",
    "    df_list = [df] * n\n",
    "    duplicated_df = pd.concat(df_list, ignore_index=True)\n",
    "    return duplicated_df\n",
    "\n",
    "\n",
    "duplicated_df = duplicate_rows(df, 10)\n",
    "duplicated_df.to_json(\"data/fool/train/train_generate.json\", indent=2, force_ascii=False, orient=\"records\")\n",
    "df.to_json(\"data/fool/dev/dev_generate.json\", indent=2, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "for i in range(len(data[\"inputs\"])):\n",
    "    data[\"inputs\"][i] = data[\"inputs\"][i] + \", \" + data[\"outputs\"][i]\n",
    "    data[\"outputs\"][i] = 1\n",
    "df1 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"inputs\": [\n",
    "        \"赵客缦胡缨\",\n",
    "        \"银鞍照白马\",\n",
    "        \"十步杀一人\",\n",
    "        \"事了拂衣去\",\n",
    "        \"闲过信陵饮\",\n",
    "        \"将炙啖朱亥\",\n",
    "        \"三杯吐然诺\",\n",
    "        \"眼花耳热后\",\n",
    "        \"救赵挥金槌\",\n",
    "        \"千秋二壮士\",\n",
    "        \"纵死侠骨香\",\n",
    "        \"谁能书阁下\",\n",
    "    ],\n",
    "    \"outputs\": [\n",
    "        \"吴钩霜雪明\",\n",
    "        \"飒沓如流星\",\n",
    "        \"千里不留行\",\n",
    "        \"深藏身与名\",\n",
    "        \"脱剑膝前横\",\n",
    "        \"持觞劝侯嬴\",\n",
    "        \"五岳倒为轻\",\n",
    "        \"意气素霓生\",\n",
    "        \"邯郸先震惊\",\n",
    "        \"烜赫大梁城\",\n",
    "        \"不惭世上英\",\n",
    "        \"白首太玄经\",\n",
    "    ],\n",
    "}\n",
    "for i in range(len(data[\"inputs\"])):\n",
    "    data[\"inputs\"][i] = data[\"inputs\"][i] + \", \" + data[\"outputs\"][len(data[\"inputs\"]) - 1 - i]\n",
    "for i in range(len(data[\"outputs\"])):\n",
    "    data[\"outputs\"][i] = 0\n",
    "df2 = pd.DataFrame(data)\n",
    "df2\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "def duplicate_rows(df, n):\n",
    "    df_list = [df] * n\n",
    "    duplicated_df = pd.concat(df_list, ignore_index=True)\n",
    "    return duplicated_df\n",
    "\n",
    "\n",
    "duplicated_df = duplicate_rows(df, 10)\n",
    "duplicated_df.to_json(\"data/fool/train/train_classify.json\", indent=2, force_ascii=False, orient=\"records\")\n",
    "df.to_json(\"data/fool/dev/dev_classify.json\", indent=2, force_ascii=False, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function(a, **kwargs_load_data):\n",
    "    print(kwargs_load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kwargs_load_data': 'abc'}\n"
     ]
    }
   ],
   "source": [
    "my_function(1, kwargs_load_data=\"abc\")\n",
    "# TypeError: my_function() got an unexpected keyword argument 'kwargs_load_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1: 1} or dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"{1: 1}\"下一\\'\\'句是?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"\\\"{a}\\\"下一''句是?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/Qwen/Qwen2.5-0.5B-Instruct/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['inputs', 'outputs'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['inputs', 'outputs'],\n",
      "        num_rows: 24\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 假设你已经加载了数据\n",
    "df_train = pd.read_json(\"data/fool/train/train_classify.json\", lines=False)\n",
    "df_dev = pd.read_json(\"data/fool/dev/dev_classify.json\", lines=False)\n",
    "\n",
    "# 将 Pandas DataFrame 转换为 Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "dev_dataset = Dataset.from_pandas(df_dev)\n",
    "\n",
    "# 可选：将训练集和验证集组合成一个 DatasetDict\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": dev_dataset})\n",
    "\n",
    "# 打印数据集信息\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'train_file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabc/def\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'train_file_path'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"your-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    train_file_path=\"abc/def\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"rouge\", cache_dir=\"outputs/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:59:24,284\u001b[32m <INFO> Configuration: 🔧 Custom attribute: deepspeed_config=None\u001b[0m\n",
      "2025-04-21 19:59:24,287\u001b[32m <INFO> Configuration: 🔧 Custom attribute: num_classification=2\u001b[0m\n",
      "2025-04-21 19:59:24,288\u001b[32m <INFO> Configuration: 🔧 Custom attribute: text_type=ORI\u001b[0m\n",
      "2025-04-21 19:59:24,289\u001b[32m <INFO> Configuration: 🔧 Custom attribute: part=all\u001b[0m\n",
      "2025-04-21 19:59:24,290\u001b[32m <INFO> Configuration: 🔧 Custom attribute: hf_generation_config_file=./configs/generate_config.json\u001b[0m\n",
      "2025-04-21 19:59:24,436 <DEBUG> TextDataset: ⏳ Loading TRAINING dataset ...\u001b[0m\n",
      "2025-04-21 19:59:24,437 <DEBUG> TextDataset: Model max length: 512\u001b[0m\n",
      "Index(['inputs', 'outputs'], dtype='object')\n",
      "==============================TRAINING==============================\n",
      "### Input: \n",
      "赵客缦胡缨, 吴钩霜雪明\n",
      "### Output: \n",
      "[1]\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a06a92667d245f184ba6c6d48662636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize TRAINING input texts:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:59:24,578 <DEBUG> TextDataset: ⌛ Loading TRAINING data takes 0.14 sec.\u001b[0m\n",
      "2025-04-21 19:59:24,580\u001b[32m <INFO> [toolkit]: Total TRAINING data: 240\u001b[0m\n",
      "2025-04-21 19:59:24,581\u001b[32m <INFO> [toolkit]: Max length of input: 13\u001b[0m\n",
      "2025-04-21 19:59:24,581\u001b[32m <INFO> [toolkit]: Max length of label: 1\u001b[0m\n",
      "2025-04-21 19:59:24,585\u001b[32m <INFO> [toolkit]: ✂️  Truncating TRAINING data: cnt=0, input_len=13, label_len=1.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from toolkit.nlp import TextDataset, NLPTrainingConfig\n",
    "from utils.load_data_fn import load_data_fn4classify, load_data_fn4generate\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = NLPTrainingConfig.load(\"outputs/fool/bert-base-chinese/ORI/train_classify-dev_classify-None/all/baseline/3/16/2e-05/2/train_config.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/jjwang/pretrained/bert-base-chinese/\")\n",
    "train_dataset = TextDataset.from_file(\n",
    "    tokenizer=tokenizer,\n",
    "    load_data_fn=load_data_fn4generate if config.task_type == \"generate\" else load_data_fn4classify,\n",
    "    split=\"TRAINING\",\n",
    "    configs=config,\n",
    "    config_load_data=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_inputs': {'input_ids': [101, 6627, 2145, 100, 5529, 5366, 117, 1426, 7174, 7458, 7434, 3209, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: <toolkit.nlp.data.TextDataset object at 0x7fbc23e150d0>\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "DatasetDict({\"train\": train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
